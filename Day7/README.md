## ğŸ“… Day 7 â€” Tensor Operations & Autograd

Today, I explored the powerful and essential **Tensor** data structure, which is the backbone of most deep learning frameworks.

### ğŸ” What I Learned:
- ğŸ”¢ **Tensor Arithmetic Operations**: Performed element-wise operations like addition, subtraction, multiplication, and division.
- ğŸ”„ **Tensor Manipulations**: Explored reshaping, slicing, and broadcasting techniques.
- ğŸ§  **Autograd in PyTorch**:
  - Understood how `requires_grad` enables automatic differentiation.
  - Used `.backward()` to compute gradients.
  - Learned how to **detach** gradients when needed to prevent tracking in computation graphs.

### ğŸ“Œ Key Takeaway:
Autograd is a powerful tool in PyTorch that automates the computation of gradients â€” crucial for backpropagation in neural networks. It significantly simplifies the process of training models.

### ğŸ“ Resources:
All related PDFs and hands-on activities for Day 7 are available in this directory:
